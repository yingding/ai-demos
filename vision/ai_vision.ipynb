{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ddd50c",
   "metadata": {},
   "source": [
    "# About the vision features\n",
    "\n",
    "### Visual features\n",
    "\n",
    "Once you've initialized an `ImageAnalysisClient`, you need to select one or more visual features to analyze. The options are specified by the enum class `VisualFeatures`. The following features are supported:\n",
    "\n",
    "1. `VisualFeatures.CAPTION` ([Examples](#generate-an-image-caption-for-an-image-file) | [Samples](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/vision/azure-ai-vision-imageanalysis/samples)): Generate a human-readable sentence that describes the content of an image.\n",
    "1. `VisualFeatures.READ` ([Examples](#extract-text-from-an-image-file) | [Samples](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/vision/azure-ai-vision-imageanalysis/samples)): Also known as Optical Character Recognition (OCR). Extract printed or handwritten text from images. **Note**: For extracting text from PDF, Office, and HTML documents and document images, use the Document Intelligence service with the [Read model](https://learn.microsoft.com/azure/ai-services/document-intelligence/concept-read?view=doc-intel-4.0.0). This model is optimized for text-heavy digital and scanned documents with an asynchronous REST API that makes it easy to power your intelligent document processing scenarios. This service is separate from the Image Analysis service and has its own SDK.\n",
    "1. `VisualFeatures.DENSE_CAPTIONS` ([Samples](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/vision/azure-ai-vision-imageanalysis/samples)): Dense Captions provides more details by generating one-sentence captions for up to 10 different regions in the image, including one for the whole image. \n",
    "1. `VisualFeatures.TAGS` ([Samples](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/vision/azure-ai-vision-imageanalysis/samples)): Extract content tags for thousands of recognizable objects, living beings, scenery, and actions that appear in images.\n",
    "1. `VisualFeatures.OBJECTS` ([Samples](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/vision/azure-ai-vision-imageanalysis/samples)): Object detection. This is similar to tagging, but focused on detecting physical objects in the image and returning their location.\n",
    "1. `VisualFeatures.SMART_CROPS` ([Samples](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/vision/azure-ai-vision-imageanalysis/samples)): Used to find a representative sub-region of the image for thumbnail generation, with priority given to include faces.\n",
    "1. `VisualFeatures.PEOPLE` ([Samples](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/vision/azure-ai-vision-imageanalysis/samples)): Detect people in the image and return their location.\n",
    "\n",
    "For more information about these features, see [Image Analysis overview](https://learn.microsoft.com/azure/ai-services/computer-vision/overview-image-analysis?tabs=4-0), and the [Concepts](https://learn.microsoft.com/azure/ai-services/computer-vision/concept-tag-images-40) page.\n",
    "\n",
    "### Reference:\n",
    "* https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/vision/azure-ai-vision-imageanalysis#visual-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3eea9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = {\n",
    "    **dotenv_values(\"./envs/aivision.env\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e6c30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_endpoint = config[\"AZURE_VISION_ENDPOINT\"]\n",
    "vision_key = config[\"AZURE_VISION_KEY\"]\n",
    "\n",
    "vision_client = ImageAnalysisClient(endpoint=vision_endpoint, credential=AzureKeyCredential(vision_key))\n",
    "\n",
    "def get_multimodal_embedding(image_url, conf_threshold=0.6):\n",
    "    response = requests.get(image_url)\n",
    "    image_data = response.content\n",
    "\n",
    "    result = vision_client.analyze(\n",
    "        image_data=image_data,\n",
    "        visual_features=[VisualFeatures.DENSE_CAPTIONS, VisualFeatures.TAGS, VisualFeatures.CAPTION],\n",
    "        # model_version=\"2023-10-01\",\n",
    "        # model_version=\"latest\"\n",
    "    )\n",
    "    # debug output of the result structure\n",
    "    # print(json.dumps(result.as_dict(), indent=4))\n",
    "\n",
    "    # Extract embeddings (vector representation)\n",
    "    if not hasattr(result, 'embedding') or result.embedding is None:\n",
    "        embedding_vector = None\n",
    "    else:\n",
    "        embedding_vector = result.embedding.vector\n",
    "    \n",
    "    # Additional metadata (optional)\n",
    "    tags = [tag.name for tag in result.tags.list if tag.confidence >= conf_threshold] if result.tags else []\n",
    "    dens_captions = [caption.text for caption in result.dense_captions.list if caption.confidence >= conf_threshold] if result.dense_captions else []\n",
    "    \n",
    "    # not output the confidence\n",
    "    caption = result.caption.text if result.caption else \"\"\n",
    "\n",
    "    return {\n",
    "        \"embedding\": embedding_vector,\n",
    "        \"tags\": tags,\n",
    "        \"dens_captions\": dens_captions,\n",
    "        \"caption\": caption  \n",
    "    }\n",
    "    \n",
    "    # return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df450b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embedding': None,\n",
       " 'tags': ['bird',\n",
       "  'animal',\n",
       "  'parrot',\n",
       "  'parakeet',\n",
       "  'budgie',\n",
       "  'perched',\n",
       "  'feather',\n",
       "  'green'],\n",
       " 'dens_captions': ['a close up of a bird',\n",
       "  \"a close up of a parrot's beak\",\n",
       "  \"a close-up of a bird's foot\",\n",
       "  \"a close up of a bird's face\",\n",
       "  'a close up of a bird'],\n",
       " 'caption': 'a green bird with red beak'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url = \"https://images.pexels.com/photos/1661179/pexels-photo-1661179.jpeg?cs=srgb&dl=pexels-roshan-kamath-1661179.jpg&fm=jpg\"\n",
    "\n",
    "result = get_multimodal_embedding(image_url, conf_threshold=0.8)\n",
    "# print(\"Multimodal embedding:\", embedding)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b518044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print result to json\n",
    "# import json\n",
    "# print(json.dumps(result.as_dict(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e930ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search3.12winpip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
