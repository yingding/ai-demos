{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure\n",
    "!pip install base64\n",
    "!pip install openai\n",
    "!pip install re\n",
    "!pip install fitz\n",
    "!pip install typing\n",
    "!pip install PyMuPDF\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79639699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd567eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "azure_doc_intelligence_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "\n",
    "client = DocumentAnalysisClient(azure_doc_intelligence_endpoint, AzureKeyCredential(doc_intelligence_key))\n",
    "\n",
    "input_filepath = \"\"#insert path of page_1.pdf here\n",
    "with open(input_filepath, \"rb\") as f:\n",
    "    poller = client.begin_analyze_document(\"prebuilt-read\", document=f)\n",
    "    result = poller.result()\n",
    "\n",
    "full_text = \"\"\n",
    "for page in result.pages:\n",
    "    for line in page.lines:\n",
    "        full_text += line.content + \"\\n\"\n",
    "\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508be173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import fitz\n",
    "\n",
    "input_filepath = \"\"#insert path of page_1.pdf here\n",
    "doc = fitz.open(input_filepath)\n",
    "page = doc[0]\n",
    "pix = page.get_pixmap(dpi=150)\n",
    "pix.save(\"page1.png\")\n",
    "\n",
    "with open(\"page1.png\", \"rb\") as image_file:\n",
    "    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5153243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "\n",
    "prompt = \"\"\"You are an intelligent document analyst.\n",
    "Given this document page, identify all images, figures or tables.\n",
    "For each figure:\n",
    "1. Provide a detailed description of what the figure or image shows.\n",
    "2. For the location just provide what the next few lines of text say, be precise (at least 2 sentences)!. If you cant find sentences afterwards (if the figure is at the end of the page) provide the previous sentences.\n",
    "\n",
    "Just return the structured list for each image/figure/table based on what you found ALWAYS use this structure:\n",
    "[Description:...\n",
    "Location:...]\n",
    ".\"\"\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = f\"\"\"You are a document editor.\n",
    "\n",
    "Given a full document and a block of sentences that appear shortly after a figure, your job is to find where in the document these sentences appear or match most closely, and insert the following figure description **just before** them.\n",
    "Respond with the new version of the document that has the description inserted in the correct place.\n",
    "Insert the figure description and add **Figure description** before it.\n",
    "---\n",
    "\n",
    "Figure Description and Location:\n",
    "{result}\n",
    "\n",
    "---\n",
    "\n",
    "Document:\n",
    "{full_text}\n",
    ".\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587bd489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut out figure from image\n",
    "import requests\n",
    "import time\n",
    "from PIL import Image\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "AZURE_CV_ENDPOINT = os.getenv(\"AZURE_CV_ENDPOINT\")\n",
    "AZURE_CV_KEY = os.getenv(\"AZURE_CV_KEY\")\n",
    "IMAGE_PATH = \"page1.png\"\n",
    "\n",
    "# The location of the figure in the document\n",
    "#to make generic extract the location from the result from first analysis\n",
    "gpt_location = \"\"\"\n",
    "1) This paper provides an in-depth analysis of major object detectors in both categories single and two stage detectors. Furthermore, we take historic look to the evolution of these methods.\n",
    "2) We present a detailed evaluation of the landmark backbone architectures and lightweight models. We could not find any paper which provides a broad overview of both these topics.\n",
    "\"\"\"\n",
    "\n",
    "with open(IMAGE_PATH, \"rb\") as f:\n",
    "    img_data = f.read()\n",
    "\n",
    "headers = {\n",
    "    \"Ocp-Apim-Subscription-Key\": AZURE_CV_KEY,\n",
    "    \"Content-Type\": \"application/octet-stream\"\n",
    "}\n",
    "\n",
    "ocr_url = AZURE_CV_ENDPOINT + \"vision/v3.2/read/analyze\"\n",
    "response = requests.post(ocr_url, headers=headers, data=img_data)\n",
    "operation_url = response.headers[\"Operation-Location\"]\n",
    "\n",
    "while True:\n",
    "    result = requests.get(operation_url, headers={\"Ocp-Apim-Subscription-Key\": AZURE_CV_KEY}).json()\n",
    "    if result[\"status\"] == \"succeeded\":\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "#Find best-matching OCR line\n",
    "lines = []\n",
    "for read_result in result[\"analyzeResult\"][\"readResults\"]:\n",
    "    for line in read_result[\"lines\"]:\n",
    "        lines.append(line)\n",
    "\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "best_line = max(lines, key=lambda l: similarity(gpt_location, l[\"text\"]))\n",
    "coords = best_line[\"boundingBox\"]\n",
    "\n",
    "x_vals = coords[::2]\n",
    "y_vals = coords[1::2]\n",
    "left = min(x_vals)\n",
    "right = max(x_vals)\n",
    "bottom = min(y_vals)\n",
    "top = max(0, bottom - 700)\n",
    "\n",
    "image = Image.open(IMAGE_PATH)\n",
    "cropped = image.crop((left, top, right, bottom))\n",
    "cropped.save(\"figure_from_azure_ocr.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
