{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure\n",
    "!pip install base64\n",
    "!pip install openai\n",
    "!pip install re\n",
    "!pip install fitz\n",
    "!pip install typing\n",
    "!pip install PyMuPDF\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79639699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd567eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "\n",
    "azure_doc_intelligence_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "\n",
    "client = DocumentAnalysisClient(azure_doc_intelligence_endpoint, AzureKeyCredential(doc_intelligence_key))\n",
    "\n",
    "input_filepath = \"\"#insert path of D365_ERP_Newsletter_Nov2020.pdf here\n",
    "with open(input_filepath, \"rb\") as f:\n",
    "    poller = client.begin_analyze_document(\"prebuilt-read\", document=f)\n",
    "    result = poller.result()\n",
    "\n",
    "full_text = \"\"\n",
    "for page in result.pages:\n",
    "    for line in page.lines:\n",
    "        full_text += line.content + \"\\n\"\n",
    "\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508be173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import fitz\n",
    "\n",
    "input_filepath = \"\"#insert path ofD365_ERP_Newsletter_Nov2020.pdf here\n",
    "doc = fitz.open(input_filepath)\n",
    "page = doc[0]\n",
    "pix = page.get_pixmap(dpi=150)\n",
    "pix.save(\"D365_ERP_Newsletter_Nov2020.pdf.png\")\n",
    "\n",
    "with open(\"D365_ERP_Newsletter_Nov2020.pdf.png\", \"rb\") as image_file:\n",
    "    base64_image = base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5153243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "\n",
    "\n",
    "prompt = \"\"\"You are an intelligent document analyst.\n",
    "Given this document page, identify all images, figures or tables.\n",
    "For each figure:\n",
    "1. Provide a detailed description of what the figure or image shows. Be precise but don't invent something you don't see. If the figure contains text also provide the text\n",
    "2. For the location just provide what the next few lines of text say after the found figure, be precise (at least 2 sentences)!. If you cant find sentences afterwards (if the figure is at the end of the page) provide the previous sentences.\n",
    "\n",
    "Just return the structured list for each image/figure/table based on what you found ALWAYS use this structure:\n",
    "[Description:...\n",
    "Location:...]\n",
    "\n",
    "write the description in german\n",
    ".\"\"\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "analytics_result = response.choices[0].message.content\n",
    "\n",
    "print(analytics_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a83b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"You are a document editor.\n",
    "\n",
    "Given a full document and a block of sentences that appear shortly after a figure, your job is to find where in the document these sentences appear or match most closely, and insert the following figure description **just before** them.\n",
    "Respond with the new version of the document that has the description inserted in the correct place.\n",
    "Insert only the figure description not the location information and add **Figure description** before it.\n",
    "---\n",
    "\n",
    "The following is the information about the description of the figure as well as the location. \n",
    "Only insert the description in the document, not the location information.\n",
    "{result}\n",
    "\n",
    "---\n",
    "\n",
    "Document:\n",
    "{full_text}\n",
    ".\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587bd489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut out figure from image\n",
    "import requests\n",
    "import time\n",
    "from PIL import Image\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "AZURE_CV_ENDPOINT = os.getenv(\"AZURE_CV_ENDPOINT\")\n",
    "AZURE_CV_KEY = os.getenv(\"AZURE_CV_KEY\")\n",
    "IMAGE_PATH = \"page1.png\"\n",
    "\n",
    "# The location of the figure in the document\n",
    "#to make generic extract the location from the result from first analysis\n",
    "gpt_location = \"\"\"\n",
    "1) This paper provides an in-depth analysis of major object detectors in both categories single and two stage detectors. Furthermore, we take historic look to the evolution of these methods.\n",
    "2) We present a detailed evaluation of the landmark backbone architectures and lightweight models. We could not find any paper which provides a broad overview of both these topics.\n",
    "\"\"\"\n",
    "\n",
    "with open(IMAGE_PATH, \"rb\") as f:\n",
    "    img_data = f.read()\n",
    "\n",
    "headers = {\n",
    "    \"Ocp-Apim-Subscription-Key\": AZURE_CV_KEY,\n",
    "    \"Content-Type\": \"application/octet-stream\"\n",
    "}\n",
    "\n",
    "ocr_url = AZURE_CV_ENDPOINT + \"vision/v3.2/read/analyze\"\n",
    "response = requests.post(ocr_url, headers=headers, data=img_data)\n",
    "operation_url = response.headers[\"Operation-Location\"]\n",
    "\n",
    "while True:\n",
    "    result = requests.get(operation_url, headers={\"Ocp-Apim-Subscription-Key\": AZURE_CV_KEY}).json()\n",
    "    if result[\"status\"] == \"succeeded\":\n",
    "        break\n",
    "    time.sleep(1)\n",
    "\n",
    "#Find best-matching OCR line\n",
    "lines = []\n",
    "for read_result in result[\"analyzeResult\"][\"readResults\"]:\n",
    "    for line in read_result[\"lines\"]:\n",
    "        lines.append(line)\n",
    "\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "best_line = max(lines, key=lambda l: similarity(gpt_location, l[\"text\"]))\n",
    "coords = best_line[\"boundingBox\"]\n",
    "\n",
    "x_vals = coords[::2]\n",
    "y_vals = coords[1::2]\n",
    "left = min(x_vals)\n",
    "right = max(x_vals)\n",
    "bottom = min(y_vals)\n",
    "top = max(0, bottom - 700)\n",
    "\n",
    "image = Image.open(IMAGE_PATH)\n",
    "cropped = image.crop((left, top, right, bottom))\n",
    "cropped.save(\"figure_from_azure_ocr.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
